{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas==1.3.5 transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -r old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT on long texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we explore different approach to overcome one of the main limitation of BERT (which stands for Bidirectional Encoder Representations from Transformers), the ability to process long document. In fact BERT can only be applied on text that have less than 512 token after tokenization with the Bert Tokenizer.\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "We will implement [this paper, which introduce a new method to deal with Long Documents : RoBERT (Recurrence over BERT)](https://arxiv.org/abs/1910.10781).\n",
    "\n",
    "\n",
    "\n",
    "We will also implement [this paper, which introduce diferents methods to deal with Long Documents and BERT](https://arxiv.org/abs/1905.05583) to see if the RoBERT paper bring some significative improvement on the classification of Long Texts with BERT.\n",
    "\n",
    "This paper introduce 2 main approaches, the **Truncation methods** and the **Hierarchical methods** :\n",
    " * Truncation methods\n",
    "   * head-only\n",
    "   * tail-only\n",
    "   * head+tail\n",
    " * Hierarchical methods\n",
    "   * mean pooling\n",
    "   * max pooling\n",
    " \n",
    "The Truncation methods applies to the input of the BERT model (the Tokens), while the Hierarchical methods applies to the ouputs of the Bert model (the embbeding), we will go into more detail in the respective parts\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "The goal of the original RoBERT article was to solve the following problem: BERT has a fixed input token count; how can we use its power on long texts. This notebook implements the same approach using HuggingFace's `transformers` library and `pytorch`.\n",
    "\n",
    "The dataset used is the *US Consumer Finance Complaints* available on [Kaggle](https://www.kaggle.com/cfpb/us-consumer-finance-complaints).\n",
    "\n",
    "Basically, the article goes as follows:\n",
    "1. Read the data and do some basic preprocessing\n",
    "2. Break the documents into smaller segments with a number of tokens that can be handled by BERT\n",
    "3. Fine-tune BERT on those segments using a classification head\n",
    "4. Combine the segments of each document by using an LSTM. The fixed output size of the LSTM can be used by a single fully connected layer for the final classification.\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "For code clarity, we separated out some parts of the code into python scripts that are fully commented. They will be referenced throughout the notebook.\n",
    "\n",
    "Here is a graph that represents the differents Interaction between the differents Classes :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/Class_Interactions.png](img/Class_Interactions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simone.mattioli/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, BertTokenizer, RobertaModel, BertModel, AdamW# get_linear_schedule_with_warmup\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "from RoBERT import RoBERT_Model\n",
    "import os\n",
    "from BERT_Hierarchical import BERT_Hierarchical_Model\n",
    "\n",
    "\n",
    "\n",
    "#### IMPORT CUSTOMIZZATE\n",
    "from Custom_Dataset_Class_Path import ConsumerComplaintsDataset1\n",
    "from Bert_Classification_Custom import Bert_Classification_Model\n",
    "from utils_Custom import *\n",
    "\n",
    "#### IMPORT ORIGINALI\n",
    "# from utils import *\n",
    "# from Custom_Dataset_Class import ConsumerComplaintsDataset1\n",
    "# from Bert_Classification import Bert_Classification_Model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Quadro RTX 6000\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this work was retrieved from kaggle as said in the paper, these are consumer complaints about financial products and services which are sent by the CFPB (Consumer FinancialProtection  Bureau)  to  the  company  for  answer.\n",
    "\n",
    "The  dataset  consists  of  555957  rows  and  18columns. \n",
    "\n",
    "As our model attempts to predict which product the complaint is about, we only used the consumer-complaint-narrative and product columns.\n",
    "* comsumer-complaint-narrative:  contains the consumer complaint in text format.\n",
    "* product:  label of the product concerned by the complaint\\\n",
    "\n",
    "The final dataset used in this work consists of 555957 rows and 2 columns (one column for the texts and the other for the labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "df=pd.read_csv(\"./us-consumer-finance-complaints/consumer_complaints.csv\")\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "train_raw = df[df.consumer_complaint_narrative.notnull()]\n",
    "print('Number of training sentences with complain narrative not null: {:,}\\n'.format(train_raw.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "train_raw.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw.consumer_complaint_narrative.apply(lambda x: len(x.split()) if len(x.split())<800 else 800).plot(kind='hist', title=\"nombre d'ocurence par nombre de mots dans un commentaire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw['len_txt'] =train_raw.consumer_complaint_narrative.apply(lambda x: len(x.split()))\n",
    "train_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only the row with number of words greater than 250:\n",
    "train_raw = train_raw[train_raw.len_txt >249]\n",
    "train_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only the column 'consumer_complaint_narrative' and 'product'\n",
    "train_raw = train_raw[['consumer_complaint_narrative', 'product']]\n",
    "train_raw.reset_index(inplace=True, drop=True)\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group similar products\n",
    "train_raw.at[train_raw['product'] == 'Credit reporting', 'product'] = 'Credit reporting, credit repair services, or other personal consumer reports'\n",
    "train_raw.at[train_raw['product'] == 'Credit card', 'product'] = 'Credit card or prepaid card'\n",
    "train_raw.at[train_raw['product'] == 'Prepaid card', 'product'] = 'Credit card or prepaid card'\n",
    "train_raw.at[train_raw['product'] == 'Payday loan', 'product'] = 'Payday loan, title loan, or personal loan'\n",
    "train_raw.at[train_raw['product'] == 'Virtual currency', 'product'] = 'Money transfer, virtual currency, or money service'\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the different classes\n",
    "for l in np.unique(train_raw['product']):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw=train_raw.rename(columns = {'consumer_complaint_narrative':'text', 'product':'label'})\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and segmentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing:\n",
    "The preprocessing step goes as follows:\n",
    "1. Remove all documents with fewer than 250 tokens. We want to concentrate only on long texts\n",
    "2. Consolidate the classes by combining those that are similar. (e.g.: \"Credit card\" or \"prepaid card\" complaints) :\n",
    " * Credit reporting‘ to ‘Credit reporting, credit repair services, or other personal consumerreports‘.\n",
    " * ‘Credit card‘ to ‘Credit card or prepaid card‘.\n",
    " * ‘Payday loan‘ to ‘Payday loan, title loan or personal loan‘1\n",
    " * ‘Virtual currency‘ to ‘Money transfer, virtual currency or money servic\n",
    "3. Remove all non-word characters\n",
    "4. Encode the labels\n",
    "5. Split the dataset in train set (80%) and validation set (20%).\n",
    "\n",
    "### 2. Segmentation and tokenization:\n",
    "First, each complaint is split into 200-token chunk with an overlap of 50 between each of them. This means that the last 50 tokens of a segment are the first 50 of the next segment.  \n",
    "Then, each segment is tokenized using BERT's tokenizer. This is needed for two main reasons:\n",
    "1. BERT's vocabulary is not made of just English words, but also subwords and single characters\n",
    "2. BERT does not take raw string as inputs. It needs:\n",
    "    - token ids: those values allow BERT to retrieve the tensor representation of a token\n",
    "    - input mask: a tensor of 0s and 1s that shows whether a token should be ignored (0) or not (1) by BERT\n",
    "    - segment ids: those are used to tell BERT what tokens form the first sentence and the second sentence (in the next sentence prediction task)  \n",
    "    \n",
    "The parameter `MAX_SEQ_LENGTH` ensures that any tokenized sentence longer that that number (200 in this case) will be truncated.  \n",
    "Each returned segment is given the same class as the document containing it.\n",
    "\n",
    "PyTorch offers the `Dataset` and `DataLoader` classes that make it easier to group the data reading and preparation operations while decreasing the memory usage in case the dataset is large.  \n",
    "We implemented the above steps in the `Custom_Dataset_Class.py` file. Our dataset class `ConsumerComplaintsDataset1` has a constructor (`__init__`) taking the necessary parameters to load the .csv file, segment the documents, and then tokenize them. It also preprocesses the data as explained in the preprocessing section.    \n",
    "The two other important methods are the following:\n",
    "- `__len__` returns the number of documents\n",
    "- `__getitem__` is the method where most of the work is done. It takes a tensor of idx values and returns the tokenized data:\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "As said above, there is differents approaches for overflowing tokens, we added the differents strategies (described in [the following paper](https://arxiv.org/abs/1905.05583)) via the use of the parameter `approach` in the initialisation of Consumer Complaints Dataset class, here is the differents value of the `approach` parameter to handle that situation :\n",
    "- **all**: overflowing tokens from a document are used to create new 200 token chunk with 50 tokens overlap between them\n",
    "- **head**: overflowing tokens are truncated. Only the first 200 tokens are used.\n",
    "- **tail**: only the last 200 tokens are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE=8\n",
    "EPOCH=10\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "MAX_SIZE_DATASET=300\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-italian-xxl-uncased', do_lower_case=True)\n",
    "\n",
    "dataset=ConsumerComplaintsDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    get_popular_keys = True,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    # max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fine-tuning on the 200 tokens chunks:\n",
    "BERT is fine-tuned on the 200 tokens chunks\n",
    "\n",
    "In our implementation, we put a neural network on top of the pooled output from BERT, each BERT's input token has a embeding as an output, the embedding of the `CLS` token (the first token) corresponds to the pooled output of the all sentence input (the 200 tokens chunk in our case).\n",
    "\n",
    "The neural network is composed of a dense layer with a SoftMax activation function.\n",
    "\n",
    "This Model corresponds to the class `Bert_Classification_Model` defined in the file `Bert_Classification.py`. This class inherits from `torch.nn.Module` which is the parent of all neural network models.\n",
    "\n",
    "Then, in the function `train_loop_fun1` defined in `utils.py`, for each batch, the list of dictionaries containing the values for the token_ids, masks, token_type_ids, and targets are respectively concatenated into `torch.tensors` which are then fed into the model in order to get predictions and apply backpropagation according to the Cross Entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First segmetation approach: all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this approach, we will consider each chunk of 200 tokens as a new document, so if a document is split into 3 chunk of 200, tokens we will consider each chunks as a new document with the same label.\\\n",
    "We use this approach to fine tune BERT, so this model will be used as an input for RoBERT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/each_Chunk_as_Document.png](img/each_Chunk_as_Document.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "#decommenta per run con dati maggioli\n",
    "n_class = dataset.get_n_classes()\n",
    "print(n_class)\n",
    "model=Bert_Classification_Model(n_class).to(device)\n",
    "\n",
    "#decommenta per run con dati originali\n",
    "# model=Bert_Classification_Model().to(device)\n",
    "\n",
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                                                                     num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "prev_val_loss = 1\n",
    "log_file = open(\"log_file_all.txt\", \"w\")\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    t0 = time.time()    \n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    log_file.write(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=train_loop_fun1(train_data_loader, model, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    log_file.write(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=eval_loop_fun1(valid_data_loader, model, device)\n",
    "    avg_val_loss = np.mean(val_losses_tmp)\n",
    "    print(f\"==> evaluation : avg_loss = {avg_val_loss:.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
    "    log_file.write(f\"==> evaluation : avg_loss = {avg_val_loss:.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
    "    ac, cr=evaluate(target, output)\n",
    "    print(f\"ACCURACY: {ac}\")\n",
    "    log_file.write(f\"ACCURACY: {ac}\")\n",
    "    print(\"CLASSIFICATION_REPORT:\")\n",
    "    print(cr)\n",
    "    log_file.write(f\"CLASSIFICATION_REPORT:\\n{cr}\")\n",
    "    #print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    #log_file.write(f\"=====>\\t{tmp_evaluate}\")\n",
    "    #val_acc.append(tmp_evaluate['accuracy'])\n",
    "    #val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(\"\\t§§ model has been saved §§\")\n",
    "    torch.save(model, f\"models/best_model_{epoch+1}.pt\")    \n",
    "    \n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.array([[np.mean(x) for x in batches_losses], [np.mean(x) for x in val_losses]]).T,\n",
    "                   columns=['Training', 'Validation']).plot(title=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.array(val_acc).T,\n",
    "                   columns=['Validation']).plot(title=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Now we will experience the [Truncation strategies presented in this paper](https://arxiv.org/abs/1905.05583)\n",
    "\n",
    "such as:  \n",
    "* Tunction Method:  \n",
    "    + Head only  \n",
    "    + Tail only  \n",
    "* Hierarchical Method:  \n",
    "    + Mean pooling  \n",
    "    + Max pooling  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "The Truncation methods applies to the input of the BERT model (the Tokens)\n",
    " \n",
    "Usually, the key information of an article is at the beginning and end. We\n",
    "use two different methods of truncate text to perform BERT fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second segmentation approach: head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this approach, we will keep only the first chunk of 200 tokens for each documents, so if a document is split into 3 chunk of 200, we will only keep the first chunk and we will not keep the last two chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/Head_Truncation.png](img/Head_Truncation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE=8\n",
    "EPOCH=10\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "MAX_SIZE_DATASET=1000\n",
    "\n",
    "approach='head'\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-italian-xxl-uncased', do_lower_case=True)\n",
    "\n",
    "dataset=ConsumerComplaintsDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN,\n",
    "    approach=approach)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "\n",
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "#decommenta per run con dati maggioli\n",
    "n_class = dataset.get_n_classes()\n",
    "print(n_class)\n",
    "model=Bert_Classification_Model(n_class).to(device)\n",
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "prev_val_loss = 1\n",
    "log_file = open(\"log_file_temp.txt\", \"w\")\n",
    "for epoch in range(EPOCH):\n",
    "    t0 = time.time()    \n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    log_file.write(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=train_loop_fun1(train_data_loader, model, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    log_file.write(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=eval_loop_fun1(valid_data_loader, model, device)\n",
    "    avg_val_loss = np.mean(val_losses_tmp)\n",
    "    print(f\"==> evaluation : avg_loss = {avg_val_loss:.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
    "    log_file.write(f\"==> evaluation : avg_loss = {avg_val_loss:.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
    "    ac, cr=evaluate(target, output)\n",
    "    print(f\"ACCURACY: {ac}\")\n",
    "    log_file.write(f\"ACCURACY: {ac}\")\n",
    "    print(\"CLASSIFICATION_REPORT:\")\n",
    "    print(cr)\n",
    "    exit()\n",
    "    log_file.write(f\"CLASSIFICATION_REPORT:\\n{cr}\")\n",
    "    #print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    #log_file.write(f\"=====>\\t{tmp_evaluate}\")\n",
    "    #val_acc.append(tmp_evaluate['accuracy'])\n",
    "    #val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(\"\\t§§ model has been saved §§\")\n",
    "    torch.save(model, f\"model_head/best_model_{epoch+1}.pt\")    \n",
    "    \n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third segmetation approach: tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this approach, we will keep only the last chunk of 200 tokens for each documents, so if a document is split into 3 chunk of 200, we will only keep the last chunk and we will not keep the first two chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/Tail_Truncation.png](img/Tail_Truncation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE=8\n",
    "EPOCH=10\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "#MAX_SIZE_DATASET=1000\n",
    "approach='tail'\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-italian-xxl-uncased', do_lower_case=True)\n",
    "\n",
    "dataset=ConsumerComplaintsDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    #max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN,\n",
    "    approach=approach)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "\n",
    "n_class = dataset.get_n_classes()\n",
    "print(n_class)\n",
    "model=Bert_Classification_Model(n_class).to(device)\n",
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "log_file = open(\"log_file_tail.txt\", \"w\")\n",
    "for epoch in range(EPOCH):\n",
    "    t0 = time.time()    \n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    log_file.write(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=train_loop_fun1(train_data_loader, model, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    log_file.write(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=eval_loop_fun1(valid_data_loader, model, device)\n",
    "    avg_val_loss = np.mean(val_losses_tmp)\n",
    "    print(f\"==> evaluation : avg_loss = {avg_val_loss:.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
    "    log_file.write(f\"==> evaluation : avg_loss = {avg_val_loss:.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
    "    ac, cr=evaluate(target, output)\n",
    "    print(f\"ACCURACY: {ac}\")\n",
    "    log_file.write(f\"ACCURACY: {ac}\")\n",
    "    print(\"CLASSIFICATION_REPORT:\")\n",
    "    print(cr)\n",
    "    log_file.write(f\"CLASSIFICATION_REPORT:\\n{cr}\")\n",
    "    #print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    #log_file.write(f\"=====>\\t{tmp_evaluate}\")\n",
    "    #val_acc.append(tmp_evaluate['accuracy'])\n",
    "    #val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(\"\\t§§ model has been saved §§\")\n",
    "    torch.save(model, f\"model_tail/best_model_{epoch+1}.pt\")    \n",
    "    \n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Hierarchical methods applies to the ouputs of the Bert model (the embbeding)\n",
    "The input text is firstly divided into k = L/510 fractions, which is fed into BERT to obtain the representation of the k text fractions. The representation of each fraction is the hidden state of the `[CLS]` tokens of the last layer. Then we use mean pooling, max pooling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Pooling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this approach, we average the embedding of all k chunks across each dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/Mean_Pooling_Hierarchical.png](img/Mean_Pooling_Hierarchical.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE=8\n",
    "EPOCH=10\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "#MAX_SIZE_DATASET=1000\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "dataset=ConsumerComplaintsDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    #max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "\n",
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "pooling_method=\"mean\"\n",
    "model_hierarchical=BERT_Hierarchical_Model(pooling_method=pooling_method).to(device)\n",
    "optimizer=AdamW(model_hierarchical.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "for epoch in range(EPOCH):\n",
    "    t0 = time.time()    \n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=rnn_train_loop_fun1(train_data_loader, model_hierarchical, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=rnn_eval_loop_fun1(valid_data_loader, model_hierarchical, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")    \n",
    "    tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "    print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    val_acc.append(tmp_evaluate['accuracy'])\n",
    "    val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(f\"\\t§§ the Hierarchical {pooling_method} pooling model has been saved §§\")\n",
    "    torch.save(model_hierarchical, f\"model_hierarchical/{pooling_method}_pooling/model_{pooling_method}_pooling_epoch{epoch+1}.pt\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Pooling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this approach, we take the maximum embedding of all the k chunks across each dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/Max_Pooling_Hierarchical.png](img/Max_Pooling_Hierarchical.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE=3\n",
    "EPOCH=1\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "#MAX_SIZE_DATASET=1000\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "dataset=ConsumerComplaintsDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    #max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "\n",
    "device=\"cpu\"\n",
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "pooling_method=\"max\"\n",
    "model_hierarchical=BERT_Hierarchical_Model(pooling_method=pooling_method).to(device)\n",
    "optimizer=AdamW(model_hierarchical.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "for epoch in range(EPOCH):\n",
    "    t0 = time.time()    \n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=rnn_train_loop_fun1(train_data_loader, model_hierarchical, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=rnn_eval_loop_fun1(valid_data_loader, model_hierarchical, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")    \n",
    "    tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "    print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    val_acc.append(tmp_evaluate['accuracy'])\n",
    "    val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(f\"\\t§§ the Hierarchical {pooling_method} pooling model has been saved §§\")\n",
    "    torch.save(model_hierarchical, f\"model_hierarchical/{pooling_method}_pooling/model_{pooling_method}_pooling_epoch{epoch+1}.pt\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERT RNN classifier on top of the Fine Tuned Bert Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The input text is firstly divided into k = L/510 fractions, which is fed into the fine tuned BERT (as describe above, *cf: First segmetation approach: all*) to obtain the representation of the k text chunks. The representation of each fraction is the hidden state of the `[CLS]` tokens of the last layer.\n",
    " \n",
    "Each chunk embedding (representation) become the input of an LSTM cell, this way the order is preserved and the length of the document is not a limitation anymore because of the dynamic aspect of the LSTM that allow different variable sequence lengths (accros different batches)\n",
    "\n",
    "we then pass the last hidden state (nbDoc * 100) to a neural network with the same architecture (as describe above, we use the same neural network architecture for classification through the all notebook, [cf: 3. Fine-tuning on the 200 tokens chunks](#3.-Fine-tuning-on-the-200-tokens-chunks:)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/RoBERT.png](img/RoBERT.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "Nettoyage des données\n",
      "['Manuale del Commercio', 'Periodici', 'Leggioggi', 'Gazzetta', 'Manuale della Circolazione Stradale']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== EPOCH 1 / 10 ===============\n",
      "\n",
      "___ batch index = 0 / 3387 (0.00%), loss = 0.8812, time = 0.64 secondes ___\n",
      "___ batch index = 640 / 3387 (18.90%), loss = 0.6725, time = 431.24 secondes ___\n",
      "___ batch index = 1280 / 3387 (37.79%), loss = 0.6508, time = 444.84 secondes ___\n",
      "___ batch index = 1920 / 3387 (56.69%), loss = 0.6489, time = 431.20 secondes ___\n",
      "___ batch index = 2560 / 3387 (75.58%), loss = 0.6273, time = 436.58 secondes ___\n",
      "___ batch index = 3200 / 3387 (94.48%), loss = 0.6529, time = 429.62 secondes ___\n",
      "\n",
      "*** avg_loss : 0.65, time : ~38.0 min (2298.30 sec) ***\n",
      "\n",
      "___ batch index = 640 / 3387 (18.90%), loss = 0.6190, time = 395.36 secondes ___\n",
      "___ batch index = 1280 / 3387 (37.79%), loss = 0.6298, time = 390.85 secondes ___\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE=8\n",
    "EPOCH=10\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "MAX_SIZE_DATASET=1000\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-italian-xxl-uncased', do_lower_case=True)\n",
    "\n",
    "dataset=ConsumerComplaintsDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    #max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "\n",
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "n_class = dataset.get_n_classes()\n",
    "model=torch.load(\"model_head/best_model_10.pt\")\n",
    "\n",
    "model_rnn=RoBERT_Model(n_class=n_class, bertFineTuned=list(model.children())[0]).to(device)\n",
    "optimizer=AdamW(model_rnn.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "\n",
    "\n",
    "log_file = open(\"log_file_rnn.txt\", \"w\")\n",
    "for epoch in range(EPOCH):\n",
    "    t0 = time.time()    \n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    log_file.write(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=rnn_train_loop_fun1(train_data_loader, model_rnn, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    log_file.write(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=rnn_eval_loop_fun1(valid_data_loader, model_rnn, device)\n",
    "    avg_val_loss = np.mean(val_losses_tmp)\n",
    "    print(f\"==> evaluation : avg_loss = {avg_val_loss:.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
    "    log_file.write(f\"==> evaluation : avg_loss = {avg_val_loss:.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
    "    ac, cr=evaluate(target, output)\n",
    "    print(f\"ACCURACY: {ac}\")\n",
    "    log_file.write(f\"ACCURACY: {ac}\")\n",
    "    print(\"CLASSIFICATION_REPORT:\")\n",
    "    print(cr)\n",
    "    log_file.write(f\"CLASSIFICATION_REPORT:\\n{cr}\")\n",
    "    #print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    #log_file.write(f\"=====>\\t{tmp_evaluate}\")\n",
    "    #val_acc.append(tmp_evaluate['accuracy'])\n",
    "    #val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(\"\\t§§ model has been saved §§\")\n",
    "    torch.save(model, f\"model_rnn1/best_model_{epoch+1}.pt\")    \n",
    "    \n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.array([[np.mean(x) for x in batches_losses], [np.mean(x) for x in val_losses]]).T,\n",
    "                   columns=['Training', 'Validation']).plot(title=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.array(val_acc).T,\n",
    "                   columns=['Validation']).plot(title=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary=pd.DataFrame({\"all\":[0.74, 0.57, 0.83], \"head only\":[0.62, 0.45, 0.87], \"tail only\":[0.93, 0.75, 0.77], \"mean pooling\":[0.62, 0.47, 0.87], \"max pooling\":[0.65, 0.45, 0.87], \"RoBERT\":[0.46, 0.34, 0.91]}, index=[\"avg_loss_train\", \"avg_loss_val\", \"accuracy\"])\n",
    "summary.columns.name = 'after one epoch'\n",
    "summary.style.set_properties(\n",
    "    subset=['RoBERT'], \n",
    "    **{'font-weight': 'bold'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the RoBERT Model give the best result, so we can conclude that this Model a net State Of the Art improvement in term of the loss function and the accuracy, the second model is the head only, this make sense because we can imagine that the consumer introduce his complain within the first part of his comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"model_rnn1/model_rnn_epoch3.pt\")\n",
    "\n",
    "#model_rnn=RoBERT_Model(bertFineTuned=list(model.children())[0]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=time.time()\n",
    "output, target, val_losses_tmp=rnn_eval_loop_fun1(valid_data_loader, model, device)\n",
    "print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")    \n",
    "tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "print(f\"=====>\\t{tmp_evaluate}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_simone_mattioli",
   "language": "python",
   "name": "kernel_simone_mattioli"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
